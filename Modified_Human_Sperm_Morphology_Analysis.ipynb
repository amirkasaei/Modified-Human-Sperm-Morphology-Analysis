{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirkasaei/Modified-Human-Sperm-Morphology-Analysis/blob/main/Modified_Human_Sperm_Morphology_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL-FNKBekJLJ"
      },
      "source": [
        "# **Libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPTAJrSSw8Zk"
      },
      "source": [
        "## **Torch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSzMBrt5MdWe",
        "outputId": "1904d18a-e3f7-41cd-f35f-645f81cf6a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/158.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torcheval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9kubfRtqF3m"
      },
      "outputs": [],
      "source": [
        "pip install -q --upgrade torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qydtc0OXpRZC"
      },
      "outputs": [],
      "source": [
        "pip install -q --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eebDe5smxNoz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torcheval\n",
        "from torcheval.metrics.functional import binary_accuracy\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Normalize, Resize\n",
        "\n",
        "# PyTorch TensorBoard support\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS2fEHBUxjg8"
      },
      "source": [
        "## **Others**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eMV3y9uQWj8",
        "outputId": "fdca360e-6360-4da5-9944-ec8a50435f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.7/2.1 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -q pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0rB-cgkur0l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import copy\n",
        "import imageio\n",
        "import PIL.Image as Image\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "from tqdm import trange\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWEt-2a_1cW2"
      },
      "outputs": [],
      "source": [
        "# !pip install numba\n",
        "\n",
        "# from numba import cuda\n",
        "# device = cuda.get_current_device()\n",
        "# device.reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BjJPwyUvKLt"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "data_path = \"/content/drive/MyDrive/AI Projects/Meta-learning vision project/mhsma-dataset sperm images/mhsma/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUn5U_UukZSX"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48nZStleLlQM"
      },
      "source": [
        "## **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3BZta8VjU9O"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/amirkasaei/Modified-Human-Sperm-Morphology-Analysis/\n",
        "path=\"/content/Modified-Human-Sperm-Morphology-Analysis/mhsma-dataset/mhsma/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcXjuMm2ujRj"
      },
      "outputs": [],
      "source": [
        "size = '128'\n",
        "x_train = np.load(path + 'x_' + size + '_train.npy').astype('float32')\n",
        "y_acrosome_train = np.load(path + 'y_acrosome_train.npy').astype('float32')\n",
        "y_head_train = np.load(path + 'y_head_train.npy').astype('float32')\n",
        "y_tail_train = np.load(path + 'y_tail_train.npy').astype('float32')\n",
        "y_vacuole_train = np.load(path + 'y_vacuole_train.npy').astype('float32')\n",
        "\n",
        "x_valid = np.load(path+'x_' + size + '_valid.npy').astype('float32')\n",
        "y_acrosome_valid = np.load(path + 'y_acrosome_valid.npy').astype('float32')\n",
        "y_head_valid = np.load(path + 'y_head_valid.npy').astype('float32')\n",
        "y_tail_valid = np.load(path + 'y_tail_valid.npy').astype('float32')\n",
        "y_vacuole_valid = np.load(path + 'y_vacuole_valid.npy').astype('float32')\n",
        "\n",
        "x_test = np.load(path+'x_' + size + '_test.npy').astype('float32')\n",
        "y_acrosome_test = np.load(path + 'y_acrosome_test.npy').astype('float32')\n",
        "y_head_test = np.load(path + 'y_head_test.npy').astype('float32')\n",
        "y_tail_test = np.load(path + 'y_tail_test.npy').astype('float32')\n",
        "y_vacuole_test = np.load(path + 'y_vacuole_test.npy').astype('float32')\n",
        "\n",
        "print('x_train shape:', x_train.shape, '----- y_acrosome_train shape:', y_acrosome_train.shape)\n",
        "print('x_valid shape:', x_valid.shape, '----- y_acrosome_valid shape:', y_acrosome_valid.shape)\n",
        "print('x_test shape:', x_test.shape, '----- y_acrosome_test shape:', y_acrosome_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mq6u2TgN9RN"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(y_head_train == 0)/y_head_train.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rQKTPEBrrwn"
      },
      "outputs": [],
      "source": [
        "np.count_nonzero(y_head_test == 0)/y_head_test.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX3YRfENLpjl"
      },
      "source": [
        "## **Reshape Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5thlpHvKmTd"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
        "# y_acrosome_train = y_acrosome_train.reshape((-1,1))\n",
        "# y_head_train = y_head_train.reshape((-1,1))\n",
        "# y_tail_train = y_tail_train.reshape((-1,1))\n",
        "# y_vacuole_train = y_vacuole_train.reshape((-1,1))\n",
        "\n",
        "x_valid = x_valid.reshape((x_valid.shape[0], x_valid.shape[1], x_valid.shape[2], 1))\n",
        "# y_acrosome_valid = y_acrosome_valid.reshape((-1,1))\n",
        "# y_head_valid = y_head_valid.reshape((-1,1))\n",
        "# y_tail_valid = y_tail_valid.reshape((-1,1))\n",
        "# y_vacuole_valid = y_vacuole_valid.reshape((-1,1))\n",
        "\n",
        "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
        "# y_acrosome_test = y_acrosome_test.reshape((-1,1))\n",
        "# y_head_test = y_head_test.reshape((-1,1))\n",
        "# y_tail_test = y_tail_test.reshape((-1,1))\n",
        "# y_vacuole_test = y_vacuole_test.reshape((-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNKyge5IJQmh"
      },
      "outputs": [],
      "source": [
        "input_shape = x_train.shape[1:]\n",
        "input_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9bKlpoVMzby"
      },
      "source": [
        "## **Torch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk_DWJDjM4us"
      },
      "outputs": [],
      "source": [
        "y_acrosome_train_tensor = torch.from_numpy(y_acrosome_train)\n",
        "y_head_train_tensor = torch.from_numpy(y_head_train)\n",
        "y_tail_train_tensor = torch.from_numpy(y_tail_train)\n",
        "y_vacuole_train_tensor = torch.from_numpy(y_vacuole_train)\n",
        "\n",
        "y_acrosome_valid_tensor = torch.from_numpy(y_acrosome_valid)\n",
        "y_head_valid_tensor = torch.from_numpy(y_head_valid)\n",
        "y_tail_valid_tensor = torch.from_numpy(y_tail_valid)\n",
        "y_vacuole_valid_tensor = torch.from_numpy(y_vacuole_valid)\n",
        "\n",
        "y_acrosome_test_tensor = torch.from_numpy(y_acrosome_test)\n",
        "y_head_test_tensor = torch.from_numpy(y_head_test)\n",
        "y_tail_test_tensor = torch.from_numpy(y_tail_test)\n",
        "y_vacuole_test_tensor = torch.from_numpy(y_vacuole_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LGUZeq2SJtS"
      },
      "outputs": [],
      "source": [
        "Tensor = T.Compose([\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "PILimage = T.Compose([\n",
        "    ToPILImage()\n",
        "])\n",
        "\n",
        "Tensor224 = T.Compose([\n",
        "    ToTensor(),\n",
        "    Resize(size = (224,224), interpolation=torchvision.transforms.InterpolationMode.NEAREST_EXACT)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAJ3atnmUdOS"
      },
      "outputs": [],
      "source": [
        "x_train_tensor = torch.stack([Tensor(img) for img in x_train])\n",
        "\n",
        "x_valid_tensor = torch.stack([Tensor(img) for img in x_valid])\n",
        "\n",
        "x_test_tensor = torch.stack([Tensor(img) for img in x_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXWPjaqYpcbN"
      },
      "outputs": [],
      "source": [
        "x_train_tensor.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmOhUrG1lNrQ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(PILimage(x_train_tensor[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji24twrPU4E0"
      },
      "source": [
        "## **Data Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHaltusVcyqq"
      },
      "source": [
        "### **Mean**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTQ71V-PbmUC"
      },
      "outputs": [],
      "source": [
        "mean_train = x_train_tensor.view(1, -1).mean(dim=1)\n",
        "mean_valid = x_valid_tensor.view(1, -1).mean(dim=1)\n",
        "mean_test = x_test_tensor.view(1, -1).mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atK-WnG6XBBB"
      },
      "outputs": [],
      "source": [
        "mean_train, mean_valid, mean_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tqOkoXuc2EC"
      },
      "source": [
        "### **Standard Deviation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3i-_ru4PcFLm"
      },
      "outputs": [],
      "source": [
        "std_train = x_train_tensor.view(1, -1).std(dim=1)\n",
        "std_valid = x_valid_tensor.view(1, -1).std(dim=1)\n",
        "std_test = x_test_tensor.view(1, -1).std(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvjLKtkeYOV1"
      },
      "outputs": [],
      "source": [
        "std_train, std_valid, std_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUhTJAx9ecBx"
      },
      "source": [
        "### **Normalizarion**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3RsifoYQKQ6"
      },
      "outputs": [],
      "source": [
        "Normalizer = Normalize(128, 8)\n",
        "x_train_norm = Normalizer(x_train_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jga9XJfseN8H"
      },
      "outputs": [],
      "source": [
        "Normalizer = Normalize(mean_valid, std_valid)\n",
        "x_valid_norm = Normalizer(x_valid_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDWTnWeqQ-9J"
      },
      "outputs": [],
      "source": [
        "Normalizer = Normalize(mean_test, std_test)\n",
        "x_test_norm = Normalizer(x_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elcdgjAlYTbZ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(PILimage(x_train_norm[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJzq-l_pQrky"
      },
      "source": [
        "## **Data Augmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gzi1adbPslv"
      },
      "source": [
        "## **Equal Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXppTtQ3QfN9"
      },
      "outputs": [],
      "source": [
        "classes, class_samples = np.unique(y_head_train_tensor, return_counts=True)\n",
        "classes, class_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZAf8-AjRPXb"
      },
      "outputs": [],
      "source": [
        "false_ids = np.random.choice(np.where(y_head_train_tensor == classes[0])[0], size=class_samples[1])\n",
        "true_ids = np.where(y_head_train_tensor == classes[1])[0]\n",
        "ids = np.sort(np.concatenate((false_ids, true_ids), axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYyV1TQuR1cz"
      },
      "outputs": [],
      "source": [
        "x_train_equal = x_train_tensor[ids]\n",
        "y_head_train_equal = y_head_train_tensor[ids]\n",
        "x_train_equal.shape, y_head_train_equal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN3BVcQ4V9lh"
      },
      "outputs": [],
      "source": [
        "y_head_train_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRnSIgfP5Dq"
      },
      "source": [
        "## **RGB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8SbiiJmP9z-"
      },
      "outputs": [],
      "source": [
        "x_train_rgb = torch.stack([img.repeat(3, 1, 1) for img in x_train_tensor])\n",
        "# x_train_rgb = torch.stack([img.repeat(3, 1, 1) for img in x_train_equal])\n",
        "\n",
        "x_valid_rgb = torch.stack([img.repeat(3, 1, 1) for img in x_valid_tensor])\n",
        "\n",
        "x_test_rgb = torch.stack([img.repeat(3, 1, 1) for img in x_test_tensor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKrW1EyjHVcw"
      },
      "outputs": [],
      "source": [
        "plt.imshow(PILimage(x_train_rgb[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnlY0Ctgnbd8"
      },
      "source": [
        "## **Data Loader**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFznc2chWf4v"
      },
      "source": [
        "### **Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEoKquW7nhpl"
      },
      "outputs": [],
      "source": [
        "class MHSMADataset(Dataset):\n",
        "  def __init__(self, x, y, transform=None, target_transform=None):\n",
        "    self.img_labels = y\n",
        "    self.imgs = x\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = self.imgs[idx]\n",
        "    label = self.img_labels[idx]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdN7OTW2Hlt_"
      },
      "source": [
        "### **Default Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waA3JNJqucuv"
      },
      "outputs": [],
      "source": [
        "train_data = MHSMADataset(x=x_train_tensor, y=y_head_train_tensor)\n",
        "\n",
        "valid_data = MHSMADataset(x=x_valid_tensor, y=y_head_valid_tensor)\n",
        "\n",
        "test_data = MHSMADataset(x=x_test_tensor, y=y_head_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO_iyB-7IaMK"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = {}\n",
        "\n",
        "dataset_sizes['train'] = len(train_data)\n",
        "dataset_sizes['valid'] = len(valid_data)\n",
        "dataset_sizes['test'] = len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3K9K3SWv6v6"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNFfvSGXH4Ms"
      },
      "outputs": [],
      "source": [
        "dataloaders = {}\n",
        "\n",
        "dataloaders['train'] = train_dataloader\n",
        "dataloaders['valid'] = valid_dataloader\n",
        "dataloaders['test'] = test_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4i5l2FDVSRnH"
      },
      "outputs": [],
      "source": [
        "dataloader_sizes = {}\n",
        "\n",
        "dataloader_sizes['train'] = len(train_dataloader)\n",
        "dataloader_sizes['valid'] = len(valid_dataloader)\n",
        "dataloader_sizes['test'] = len(test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyT1bZJBH_lx"
      },
      "source": [
        "### **RGB Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Nt3sMRfQpzT"
      },
      "outputs": [],
      "source": [
        "train_data_rgb = MHSMADataset(x=x_train_rgb, y=y_head_train_tensor)\n",
        "# train_data_rgb = MHSMADataset(x=x_train_rgb, y=y_head_train_equal)\n",
        "\n",
        "valid_data_rgb = MHSMADataset(x=x_valid_rgb, y=y_head_valid_tensor)\n",
        "\n",
        "test_data_rgb = MHSMADataset(x=x_test_rgb, y=y_head_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRGCYNntRLF6"
      },
      "outputs": [],
      "source": [
        "batch_size=64\n",
        "\n",
        "train_dataloader_rgb = DataLoader(train_data_rgb, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "valid_dataloader_rgb = DataLoader(valid_data_rgb, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_dataloader_rgb = DataLoader(test_data_rgb, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKDOng-ARjMN"
      },
      "outputs": [],
      "source": [
        "dataloaders_rgb = {}\n",
        "\n",
        "dataloaders_rgb['train'] = train_dataloader_rgb\n",
        "dataloaders_rgb['valid'] = valid_dataloader_rgb\n",
        "dataloaders_rgb['test'] = test_dataloader_rgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKY7hupgvgf4"
      },
      "outputs": [],
      "source": [
        "dataloader_rgb_sizes = {}\n",
        "\n",
        "dataloader_rgb_sizes['train'] = len(train_dataloader_rgb)\n",
        "dataloader_rgb_sizes['valid'] = len(valid_dataloader_rgb)\n",
        "dataloader_rgb_sizes['test'] = len(test_dataloader_rgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hEI8s9Dfcpf"
      },
      "outputs": [],
      "source": [
        "train_batch_num = len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PENI9Um8waCF"
      },
      "outputs": [],
      "source": [
        "# train_features, train_labels = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZvqR2S8GA0X"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-EiJgnMHHI6"
      },
      "source": [
        "## **Helper Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-LzMaWHHVKE"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ONj06MTsKE"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbuFLMNxtx3G"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, dataloader, dataloader_size, num_epochs=25):\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print('-' * 20)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'valid']:\n",
        "      if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "      else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      # running_acc = 0.0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloader[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.type(torch.LongTensor).to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "          # acc = binary_accuracy(outputs.reshape(outputs.shape[0]), labels.reshape(labels.shape[0]), threshold=0.5)\n",
        "\n",
        "          # backward + optimize only if in training phase\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        # running_acc += acc\n",
        "\n",
        "        # if phase == 'train':\n",
        "        #   scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss / dataset_sizes[phase]\n",
        "      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "      print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "      # deep copy the model\n",
        "      if phase == 'valid' and epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print()\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "  # load best model weights\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTGavrOZ0DPn"
      },
      "source": [
        "### **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBOzU-Uyn_15"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, criterion, dataloader, dataloader_size, dataset_size):\n",
        "  since = time.time()\n",
        "\n",
        "  print('-' * 20)\n",
        "\n",
        "  # Each epoch has a training and validation phase\n",
        "  model.eval()   # Set model to evaluate mode\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "  # running_acc = 0.0\n",
        "\n",
        "  # Iterate over data.\n",
        "  for inputs, labels in dataloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.type(torch.LongTensor).to(device)\n",
        "\n",
        "    # forward\n",
        "    # track history if only in train\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "    # statistics\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  eval_loss = running_loss / dataset_size\n",
        "  eval_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "  print(f'test Loss: {eval_loss:.4f} Acc: {eval_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zOWCj-nts7G"
      },
      "source": [
        "## **VGG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1e3fvamGdNq"
      },
      "source": [
        "### **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNUfIL7SR4Jl",
        "outputId": "b68a4d17-fe14-4a29-fc47-9f43d0ea6baa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (1): ReLU(inplace=True)\n",
              "  (2): Dropout(p=0.5, inplace=False)\n",
              "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (4): ReLU(inplace=True)\n",
              "  (5): Dropout(p=0.5, inplace=False)\n",
              "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vgg = torchvision.models.vgg19(pretrained=True)\n",
        "vgg.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9048mBqkTuW"
      },
      "outputs": [],
      "source": [
        "# for param in vgg.features.parameters():\n",
        "#     param.require_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrnMoH6CSl-6",
        "outputId": "ddbd9b31-87ba-46e6-8fbf-86e3235f1c6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=25088, out_features=2048, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.9, inplace=False)\n",
              "  (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "  (4): Sigmoid()\n",
              "  (5): Dropout(p=0.9, inplace=False)\n",
              "  (6): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_ftrs = vgg.classifier[0].in_features\n",
        "vgg.classifier = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 2048),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.9),\n",
        "    nn.Linear(2048, 1024),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Dropout(0.9),\n",
        "    nn.Linear(1024, 2)\n",
        ")\n",
        "\n",
        "vgg.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUqtLeBIGiEK"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8fjHYzKZlaL"
      },
      "outputs": [],
      "source": [
        "vgg = vgg.to(device)\n",
        "\n",
        "learning_rate = 3e-3\n",
        "epochs = 100\n",
        "\n",
        "# criterion = nn.BCELoss()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# accuracy = metrics.accuracy_score()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = torch.optim.Adam(vgg.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.SGD(vgg.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer = torch.optim.Adadelta(vgg.parameters(), lr=learning_rate)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBUgqN7XZniN",
        "outputId": "60f95d83-28ce-48a9-fa79-ea2481389a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "--------------------\n",
            "train Loss: 0.5263 Acc: 0.2680\n",
            "valid Loss: 0.7123 Acc: 0.3167\n",
            "\n",
            "Epoch 2/100\n",
            "--------------------\n",
            "train Loss: 0.5792 Acc: 0.2590\n",
            "valid Loss: 0.6885 Acc: 0.5125\n",
            "\n",
            "Epoch 3/100\n",
            "--------------------\n",
            "train Loss: 0.5409 Acc: 0.2570\n",
            "valid Loss: 0.6838 Acc: 0.5958\n",
            "\n",
            "Epoch 4/100\n",
            "--------------------\n",
            "train Loss: 0.4911 Acc: 0.2910\n",
            "valid Loss: 0.6623 Acc: 0.7333\n",
            "\n",
            "Epoch 5/100\n",
            "--------------------\n",
            "train Loss: 0.5337 Acc: 0.2700\n",
            "valid Loss: 0.6735 Acc: 0.6917\n",
            "\n",
            "Epoch 6/100\n",
            "--------------------\n",
            "train Loss: 0.5028 Acc: 0.2730\n",
            "valid Loss: 0.6586 Acc: 0.7333\n",
            "\n",
            "Epoch 7/100\n",
            "--------------------\n",
            "train Loss: 0.5117 Acc: 0.2720\n",
            "valid Loss: 0.6630 Acc: 0.7333\n",
            "\n",
            "Epoch 8/100\n",
            "--------------------\n",
            "train Loss: 0.5261 Acc: 0.2630\n",
            "valid Loss: 0.6649 Acc: 0.7250\n",
            "\n",
            "Epoch 9/100\n",
            "--------------------\n",
            "train Loss: 0.5087 Acc: 0.2690\n",
            "valid Loss: 0.6548 Acc: 0.7250\n",
            "\n",
            "Epoch 10/100\n",
            "--------------------\n",
            "train Loss: 0.4720 Acc: 0.3050\n",
            "valid Loss: 0.6404 Acc: 0.7292\n",
            "\n",
            "Epoch 11/100\n",
            "--------------------\n",
            "train Loss: 0.4991 Acc: 0.2860\n",
            "valid Loss: 0.6517 Acc: 0.7375\n",
            "\n",
            "Epoch 12/100\n",
            "--------------------\n",
            "train Loss: 0.4886 Acc: 0.2930\n",
            "valid Loss: 0.6611 Acc: 0.7333\n",
            "\n",
            "Epoch 13/100\n",
            "--------------------\n",
            "train Loss: 0.5135 Acc: 0.2610\n",
            "valid Loss: 0.6781 Acc: 0.6750\n",
            "\n",
            "Epoch 14/100\n",
            "--------------------\n",
            "train Loss: 0.4850 Acc: 0.2790\n",
            "valid Loss: 0.6790 Acc: 0.6417\n",
            "\n",
            "Epoch 15/100\n",
            "--------------------\n",
            "train Loss: 0.5181 Acc: 0.2630\n",
            "valid Loss: 0.6942 Acc: 0.4500\n",
            "\n",
            "Epoch 16/100\n",
            "--------------------\n",
            "train Loss: 0.4677 Acc: 0.2810\n",
            "valid Loss: 0.6972 Acc: 0.4542\n",
            "\n",
            "Epoch 17/100\n",
            "--------------------\n",
            "train Loss: 0.5018 Acc: 0.2570\n",
            "valid Loss: 0.7053 Acc: 0.3583\n",
            "\n",
            "Epoch 18/100\n",
            "--------------------\n",
            "train Loss: 0.4858 Acc: 0.2670\n",
            "valid Loss: 0.6632 Acc: 0.7208\n",
            "\n",
            "Epoch 19/100\n",
            "--------------------\n",
            "train Loss: 0.4970 Acc: 0.2680\n",
            "valid Loss: 0.6906 Acc: 0.5125\n",
            "\n",
            "Epoch 20/100\n",
            "--------------------\n",
            "train Loss: 0.4974 Acc: 0.2620\n",
            "valid Loss: 0.6707 Acc: 0.6875\n",
            "\n",
            "Epoch 21/100\n",
            "--------------------\n",
            "train Loss: 0.4421 Acc: 0.2790\n",
            "valid Loss: 0.6853 Acc: 0.5583\n",
            "\n",
            "Epoch 22/100\n",
            "--------------------\n",
            "train Loss: 0.4984 Acc: 0.2800\n",
            "valid Loss: 0.6885 Acc: 0.5417\n",
            "\n",
            "Epoch 23/100\n",
            "--------------------\n",
            "train Loss: 0.4440 Acc: 0.2910\n",
            "valid Loss: 0.6734 Acc: 0.6667\n",
            "\n",
            "Epoch 24/100\n",
            "--------------------\n",
            "train Loss: 0.4459 Acc: 0.2930\n",
            "valid Loss: 0.6773 Acc: 0.6375\n",
            "\n",
            "Epoch 25/100\n",
            "--------------------\n",
            "train Loss: 0.4682 Acc: 0.2790\n",
            "valid Loss: 0.6781 Acc: 0.6667\n",
            "\n",
            "Epoch 26/100\n",
            "--------------------\n",
            "train Loss: 0.4980 Acc: 0.2770\n",
            "valid Loss: 0.6753 Acc: 0.6875\n",
            "\n",
            "Epoch 27/100\n",
            "--------------------\n",
            "train Loss: 0.4690 Acc: 0.2640\n",
            "valid Loss: 0.6725 Acc: 0.6667\n",
            "\n",
            "Epoch 28/100\n",
            "--------------------\n",
            "train Loss: 0.4484 Acc: 0.2810\n",
            "valid Loss: 0.6761 Acc: 0.6708\n",
            "\n",
            "Epoch 29/100\n",
            "--------------------\n",
            "train Loss: 0.4792 Acc: 0.2720\n",
            "valid Loss: 0.6827 Acc: 0.5833\n",
            "\n",
            "Epoch 30/100\n",
            "--------------------\n",
            "train Loss: 0.4573 Acc: 0.2810\n",
            "valid Loss: 0.7032 Acc: 0.3833\n",
            "\n",
            "Epoch 31/100\n",
            "--------------------\n",
            "train Loss: 0.4641 Acc: 0.2820\n",
            "valid Loss: 0.7056 Acc: 0.3708\n",
            "\n",
            "Epoch 32/100\n",
            "--------------------\n",
            "train Loss: 0.4968 Acc: 0.2590\n",
            "valid Loss: 0.6977 Acc: 0.4667\n",
            "\n",
            "Epoch 33/100\n",
            "--------------------\n",
            "train Loss: 0.4383 Acc: 0.2920\n",
            "valid Loss: 0.6810 Acc: 0.6042\n",
            "\n",
            "Epoch 34/100\n",
            "--------------------\n",
            "train Loss: 0.4614 Acc: 0.2670\n",
            "valid Loss: 0.6958 Acc: 0.4458\n",
            "\n",
            "Epoch 35/100\n",
            "--------------------\n",
            "train Loss: 0.4585 Acc: 0.2720\n",
            "valid Loss: 0.6957 Acc: 0.4542\n",
            "\n",
            "Epoch 36/100\n",
            "--------------------\n",
            "train Loss: 0.4568 Acc: 0.2780\n",
            "valid Loss: 0.6947 Acc: 0.4417\n",
            "\n",
            "Epoch 37/100\n",
            "--------------------\n",
            "train Loss: 0.4758 Acc: 0.2700\n",
            "valid Loss: 0.6840 Acc: 0.5667\n",
            "\n",
            "Epoch 38/100\n",
            "--------------------\n",
            "train Loss: 0.4771 Acc: 0.2680\n",
            "valid Loss: 0.6794 Acc: 0.6083\n",
            "\n",
            "Epoch 39/100\n",
            "--------------------\n",
            "train Loss: 0.4608 Acc: 0.2880\n",
            "valid Loss: 0.6872 Acc: 0.5333\n",
            "\n",
            "Epoch 40/100\n",
            "--------------------\n",
            "train Loss: 0.4459 Acc: 0.2820\n",
            "valid Loss: 0.7003 Acc: 0.4500\n",
            "\n",
            "Epoch 41/100\n",
            "--------------------\n",
            "train Loss: 0.4603 Acc: 0.2650\n",
            "valid Loss: 0.7015 Acc: 0.4250\n",
            "\n",
            "Epoch 42/100\n",
            "--------------------\n",
            "train Loss: 0.4523 Acc: 0.2790\n",
            "valid Loss: 0.6686 Acc: 0.6250\n",
            "\n",
            "Epoch 43/100\n",
            "--------------------\n",
            "train Loss: 0.4637 Acc: 0.2920\n",
            "valid Loss: 0.6485 Acc: 0.7125\n",
            "\n",
            "Epoch 44/100\n",
            "--------------------\n",
            "train Loss: 0.4624 Acc: 0.2800\n",
            "valid Loss: 0.6713 Acc: 0.5875\n",
            "\n",
            "Epoch 45/100\n",
            "--------------------\n",
            "train Loss: 0.4513 Acc: 0.2930\n",
            "valid Loss: 0.6949 Acc: 0.4958\n",
            "\n",
            "Epoch 46/100\n",
            "--------------------\n",
            "train Loss: 0.4537 Acc: 0.2810\n",
            "valid Loss: 0.6779 Acc: 0.5708\n",
            "\n",
            "Epoch 47/100\n",
            "--------------------\n",
            "train Loss: 0.4354 Acc: 0.2890\n",
            "valid Loss: 0.6592 Acc: 0.6708\n",
            "\n",
            "Epoch 48/100\n",
            "--------------------\n",
            "train Loss: 0.4497 Acc: 0.2750\n",
            "valid Loss: 0.6522 Acc: 0.7292\n",
            "\n",
            "Epoch 49/100\n",
            "--------------------\n",
            "train Loss: 0.4503 Acc: 0.2850\n",
            "valid Loss: 0.6495 Acc: 0.7042\n",
            "\n",
            "Epoch 50/100\n",
            "--------------------\n",
            "train Loss: 0.4443 Acc: 0.3050\n",
            "valid Loss: 0.6347 Acc: 0.7167\n",
            "\n",
            "Epoch 51/100\n",
            "--------------------\n",
            "train Loss: 0.4393 Acc: 0.2950\n",
            "valid Loss: 0.6555 Acc: 0.6500\n",
            "\n",
            "Epoch 52/100\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "vgg = train_model(vgg, criterion,optimizer, exp_lr_scheduler, dataloaders_rgb, dataloader_rgb_sizes, num_epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-vMk0gwGkcE"
      },
      "source": [
        "### **Evaluarte**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_E5IZWjF8i9",
        "outputId": "5812bec3-efa7-4a13-d3cc-e6608aaa1575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "test Loss: 0.6116 Acc: 0.7300\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(vgg, criterion, dataloaders_rgb, dataloader_rgb_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfFFF-NNkUbQ"
      },
      "source": [
        "## **ViT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khIgG5S3qj-K"
      },
      "source": [
        "### **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-kjwMUhnpOQ"
      },
      "outputs": [],
      "source": [
        "vit = torchvision.models.vit_b_16(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enNBPur8rRwz",
        "outputId": "b5bf965a-403c-48d0-bc8e-31019f5d15f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vit.heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxkCVvvRrd8Y"
      },
      "outputs": [],
      "source": [
        "num_ftrs = vit.heads.head.in_features\n",
        "vit.heads =  nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.9),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Dropout(0.9),\n",
        "    nn.Linear(256, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duUHrL-Ftjg-",
        "outputId": "5acf70d0-02c5-4200-c845-e8c8258725de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=768, out_features=512, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Dropout(p=0.9, inplace=False)\n",
              "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (4): Sigmoid()\n",
              "  (5): Dropout(p=0.9, inplace=False)\n",
              "  (6): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vit.heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohggQp1bqqJH"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r1hGA4GsXRx"
      },
      "outputs": [],
      "source": [
        "vit = vit.to(device)\n",
        "\n",
        "learning_rate = 3e-3\n",
        "epochs = 100\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# accuracy = metrics.accuracy_score()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "# optimizer = torch.optim.Adam(vgg.parameters(), lr=learning_rate)\n",
        "# optimizer = torch.optim.SGD(vgg.parameters(), lr=learning_rate, momentum=0.9)\n",
        "optimizer = torch.optim.Adadelta(vit.parameters(), lr=learning_rate)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aY4QWI02sgJH"
      },
      "outputs": [],
      "source": [
        "vit = train_model(vit, criterion, optimizer, exp_lr_scheduler, dataloaders_rgb, dataloader_rgb_sizes, num_epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wopM_Wh4qx3R"
      },
      "source": [
        "### **Evaluarte**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXNgMf5oqx3d",
        "outputId": "5812bec3-efa7-4a13-d3cc-e6608aaa1575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "test Loss: 0.6116 Acc: 0.7300\n"
          ]
        }
      ],
      "source": [
        "evaluate_model(vit, criterion, dataloaders_rgb, dataloader_rgb_sizes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6L_RRUPIlXC"
      },
      "source": [
        "## **CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkC3IO5OIn-w"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linrear1 = nn.Linear(in_features=16 * 30 * 30, out_features=128)\n",
        "    self.linrear2 = nn.Linear(in_features=128, out_features=32)\n",
        "    self.output = nn.Linear(in_features=32, out_features=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = self.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = x.view(-1, 16 * 30 * 30)\n",
        "    x = self.flatten(x)\n",
        "    x = self.relu(self.linrear1(x))\n",
        "    x = self.relu(self.linrear2(x))\n",
        "    x = self.sigmoid(self.output(x))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i8h11AkV2fS"
      },
      "outputs": [],
      "source": [
        "model = CNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUwZAya0YXvh"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t706vSHIULuc"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 100\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTJiJhxtVhTC"
      },
      "outputs": [],
      "source": [
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, dataloaders, dataloader_sizes, num_epochs=epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iL-FNKBekJLJ",
        "JPTAJrSSw8Zk",
        "fS2fEHBUxjg8",
        "RyLgptfAxWXL",
        "pUn5U_UukZSX",
        "48nZStleLlQM",
        "GX3YRfENLpjl",
        "H9bKlpoVMzby",
        "Ji24twrPU4E0",
        "zHaltusVcyqq",
        "2tqOkoXuc2EC",
        "nUhTJAx9ecBx",
        "-Gzi1adbPslv",
        "tdRnSIgfP5Dq",
        "ZnlY0Ctgnbd8",
        "RFznc2chWf4v",
        "BdN7OTW2Hlt_",
        "PyT1bZJBH_lx",
        "-xW5mHB5wuzX",
        "o-EiJgnMHHI6",
        "94ONj06MTsKE",
        "wTGavrOZ0DPn",
        "-zOWCj-nts7G",
        "MUqtLeBIGiEK",
        "T-vMk0gwGkcE",
        "rfFFF-NNkUbQ",
        "khIgG5S3qj-K",
        "ohggQp1bqqJH",
        "-zxNScAR7LEe",
        "DhC0fjB2mK3S",
        "ujKaeAcWjGM7",
        "r6kN5bLRjA1W",
        "uK-lXCyhHVdI",
        "J6L_RRUPIlXC"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}